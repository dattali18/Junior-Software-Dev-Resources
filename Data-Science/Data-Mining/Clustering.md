# Clustering Algorithm Summery

Here's a summary of the clustering algorithms we discussed, categorized by family of algorithm:
### Hierarchical Clustering
1. **AGNES (AGglomerative NESting):**
   - Family: Agglomerative clustering.
   - Description: Agglomerative clustering starts with each data point as a single cluster and iteratively merges the closest clusters until only one cluster remains.
   - Method: Merge the two closest clusters at each step based on a chosen distance metric.
   
2. **DIANA (DIvisive ANAlysis):**
   - Family: Divisive clustering.
   - Description: Divisive clustering starts with all data points in a single cluster and recursively splits clusters into smaller clusters until each data point is in its cluster.
   - Method: Split clusters based on the pair of points with the highest dissimilarity.

### Partitioning Clustering
1. **K-Means:**
   - Family: Partitioning clustering.
   - Description: Partitioning clustering divides the dataset into non-overlapping clusters.
   - Method: Iteratively assigns data points to the nearest cluster centroid and updates centroids based on the mean of the points assigned to each cluster.

2. **K-Medoids (PAM):**
   - Family: Partitioning clustering.
   - Description: Similar to K-Means but uses actual data points as centroids.
   - Method: Minimizes the sum of dissimilarities between data points and their closest medoids.

### Density-Based Clustering
1. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**
   - Family: Density-based clustering.
   - Description: Forms clusters based on areas of high density separated by areas of low density.
   - Method: Identifies core points with many nearby neighbors and expands clusters from them.

### Model-Based Clustering
1. **Expectation Maximization (EM) Algorithm:**
   - Family: Model-based clustering.
   - Description: Assumes that the data is generated by a mixture of several Gaussian distributions.
   - Method: Estimates the parameters of the Gaussian distributions using the EM algorithm.

Each clustering algorithm has its strengths and weaknesses, and the choice of algorithm depends on the nature of the data and the desired clustering outcome. Hierarchical clustering is useful for visualizing hierarchical relationships, while partitioning clustering methods like K-Means are efficient for large datasets. Density-based clustering like DBSCAN is robust to noise and can find arbitrarily shaped clusters, and model-based clustering like EM can handle complex data distributions.

# Clustering Evaluation

### Clustering Evaluation Metrics

#### Silhouette Score
- **Definition:** The silhouette score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).
- **Formula:** For each sample, the silhouette score is calculated as $\frac{b-a}{\max(a, b)}$, where $a$ is the mean intra-cluster distance (distance between a sample and all other points in the same cluster) and $b$ is the mean nearest-cluster distance (distance between a sample and all points in the nearest cluster that the sample is not a part of).
- **Interpretation:** The silhouette score ranges from -1 to 1. A score closer to 1 indicates that the sample is well-clustered, while a score close to -1 indicates that the sample may be assigned to the wrong cluster.

#### Rand Index (RI)
- **Definition:** The Rand index measures the similarity between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in both the true and predicted clusterings.
- **Formula:** The Rand index is calculated as the number of agreeing pairs (pairs in the same cluster in both true and predicted clusterings) plus the number of disagreeing pairs (pairs in different clusters in both true and predicted clusterings) divided by the total number of pairs.
- **Interpretation:** The Rand index ranges from 0 to 1, where 0 indicates that the two clusterings are completely different, and 1 indicates that the two clusterings are identical.

#### Adjusted Rand Index (ARI)
- **Definition:** The adjusted Rand index is a corrected-for-chance version of the Rand index that accounts for the expected similarity between random clusterings.
- **Formula:** The ARI is calculated as the Rand index adjusted for chance, taking into account the expected similarity between random clusterings.
- **Interpretation:** The ARI ranges from -1 to 1. A score of 0 indicates that the clustering is random, while a score of 1 indicates that the clustering is identical to the true clustering. Negative values indicate clustering worse than random.

These metrics are used to evaluate the quality of clustering algorithms by comparing the cluster assignments to a ground truth (if available) or by assessing the internal coherence of the clusters. The choice of metric depends on the specific characteristics of the data and the goals of the clustering analysis.