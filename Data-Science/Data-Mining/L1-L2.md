# L1 & L2

L1 and L2 regularization are techniques used to prevent overfitting in machine learning models by adding a penalty term to the loss function. This penalty term encourages the model to learn simpler patterns and avoid fitting the noise in the data.

### L1 Regularization (Lasso Regression)

L1 regularization adds the sum of the absolute values of the coefficients to the loss function. It encourages sparsity in the model, meaning it tends to set some coefficients to zero, effectively performing feature selection.

The L1 regularization term is defined as:

$$
\text{L1 regularization term} = \lambda \sum_{i=1}^{n} |w_i|
$$

where $\lambda$ is the regularization parameter and $w_i$ are the model coefficients.

### L2 Regularization (Ridge Regression)

L2 regularization adds the sum of the squares of the coefficients to the loss function. It penalizes large coefficients but does not set them exactly to zero, making it less prone to feature selection compared to L1 regularization.

The L2 regularization term is defined as:

$$
\text{L2 regularization term} = \lambda \sum_{i=1}^{n} w_i^2
$$

where $\lambda$ is the regularization parameter and $w_i$ are the model coefficients.

### Code Example (Python)

Here's an example using L1 and L2 regularization with linear regression in scikit-learn:

```python
from sklearn.linear_model import Lasso, Ridge
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# L1 regularization (Lasso)
lasso = Lasso(alpha=0.1)  # alpha is the regularization parameter lambda
lasso.fit(X_train, y_train)
lasso_predictions = lasso.predict(X_test)
lasso_mse = mean_squared_error(y_test, lasso_predictions)

# L2 regularization (Ridge)
ridge = Ridge(alpha=0.1)  # alpha is the regularization parameter lambda
ridge.fit(X_train, y_train)
ridge_predictions = ridge.predict(X_test)
ridge_mse = mean_squared_error(y_test, ridge_predictions)

print("Lasso MSE:", lasso_mse)
print("Ridge MSE:", ridge_mse)
```

In this example, we first generate synthetic data using `make_regression`. Then, we split the data into training and test sets. We use `Lasso` for L1 regularization and `Ridge` for L2 regularization, with a regularization parameter (`alpha`) of 0.1 for both. Finally, we calculate the mean squared error (MSE) for both models on the test set.